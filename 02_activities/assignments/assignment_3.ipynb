{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assigment, we will work with the *Forest Fire* data set. Please download the data from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/dataset/162/forest+fires). Extract the data files into the subdirectory: `../data/fires/` (relative to `./src/`).\n",
    "\n",
    "## Objective\n",
    "\n",
    "+ The model objective is to predict the area affected by forest fires given the features set. \n",
    "+ The objective of this exercise is to assess your ability to construct and evaluate model pipelines.\n",
    "+ Please note: the instructions are not meant to be 100% prescriptive, but instead they are a set of minimum requirements. If you find predictive performance gains by applying additional steps, by all means show them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Description\n",
    "\n",
    "From the description file contained in the archive (`forestfires.names`), we obtain the following variable descriptions:\n",
    "\n",
    "1. X - x-axis spatial coordinate within the Montesinho park map: 1 to 9\n",
    "2. Y - y-axis spatial coordinate within the Montesinho park map: 2 to 9\n",
    "3. month - month of the year: \"jan\" to \"dec\" \n",
    "4. day - day of the week: \"mon\" to \"sun\"\n",
    "5. FFMC - FFMC index from the FWI system: 18.7 to 96.20\n",
    "6. DMC - DMC index from the FWI system: 1.1 to 291.3 \n",
    "7. DC - DC index from the FWI system: 7.9 to 860.6 \n",
    "8. ISI - ISI index from the FWI system: 0.0 to 56.10\n",
    "9. temp - temperature in Celsius degrees: 2.2 to 33.30\n",
    "10. RH - relative humidity in %: 15.0 to 100\n",
    "11. wind - wind speed in km/h: 0.40 to 9.40 \n",
    "12. rain - outside rain in mm/m2 : 0.0 to 6.4 \n",
    "13. area - the burned area of the forest (in ha): 0.00 to 1090.84 \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "### Specific Tasks\n",
    "\n",
    "+ Construct four model pipelines, out of combinations of the following components:\n",
    "\n",
    "    + Preprocessors:\n",
    "\n",
    "        - A simple processor that only scales numeric variables and recodes categorical variables.\n",
    "        - A transformation preprocessor that scales numeric variables and applies a non-linear transformation.\n",
    "    \n",
    "    + Regressor:\n",
    "\n",
    "        - A baseline regressor, which could be a [K-nearest neighbours model](https://open.spotify.com/track/4R3AU2pjv8ge2siX1fVbZs?si=b2712f32da0e4358) or a simple [linear regression model](https://scikit-learn.org/stable/modules/linear_model.html)\n",
    "        - An advanced regressor of your choice (e.g., Random Forest, Neural Network, etc.)\n",
    "\n",
    "+ Evaluate tune and evaluate each of the four model pipelines. \n",
    "\n",
    "    - Select a [performance metric](https://scikit-learn.org/stable/modules/linear_model.html) out of the following options: explained variance, max error, root mean squared error (RMSE), mean absolute error (MAE), r-squared.\n",
    "    - *TIPS*: \n",
    "    \n",
    "        * Out of the suggested metrics above, [some are correlation metrics, but this is a prediction problem](https://www.tmwr.org/performance#performance). Choose wisely (and don't choose the incorrect options.) \n",
    "\n",
    "+ Select the best-performing model and explain its predictions.\n",
    "\n",
    "    - Provide local explanations.\n",
    "    - Obtain global explanations and recommend a variable selection strategy.\n",
    "\n",
    "+ Export your model as a pickle file.\n",
    "\n",
    "\n",
    "You can work on the Jupyter notebook, as this experiment is fairly short (no need to use sacred). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: shap in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.46.0)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from shap) (2.0.0)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from shap) (1.14.0)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from shap) (1.5.1)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from shap) (2.2.2)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from shap) (4.66.4)\n",
      "Requirement already satisfied: packaging>20.9 in /Users/kristina/Library/Python/3.12/lib/python/site-packages (from shap) (24.1)\n",
      "Requirement already satisfied: slicer==0.0.8 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from shap) (0.0.8)\n",
      "Requirement already satisfied: numba in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from shap) (0.60.0)\n",
      "Requirement already satisfied: cloudpickle in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from shap) (3.0.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from numba->shap) (0.43.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/kristina/Library/Python/3.12/lib/python/site-packages (from pandas->shap) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas->shap) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas->shap) (2024.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn->shap) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn->shap) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/kristina/Library/Python/3.12/lib/python/site-packages (from python-dateutil>=2.8.2->pandas->shap) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PowerTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import shap\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "\n",
    "Assuming that the files `adult.data` and `adult.test` are in `../data/adult/`, then you can use the code below to load them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ucimlrepo in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.0.7)\n",
      "Requirement already satisfied: pandas>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from ucimlrepo) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2020.12.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from ucimlrepo) (2024.6.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas>=1.0.0->ucimlrepo) (2.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/kristina/Library/Python/3.12/lib/python/site-packages (from pandas>=1.0.0->ucimlrepo) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas>=1.0.0->ucimlrepo) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas>=1.0.0->ucimlrepo) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/kristina/Library/Python/3.12/lib/python/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ucimlrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 162, 'name': 'Forest Fires', 'repository_url': 'https://archive.ics.uci.edu/dataset/162/forest+fires', 'data_url': 'https://archive.ics.uci.edu/static/public/162/data.csv', 'abstract': 'This is a difficult regression task, where the aim is to predict the burned area of forest fires, in the northeast region of Portugal, by using meteorological and other data (see details at: http://www.dsi.uminho.pt/~pcortez/forestfires).', 'area': 'Climate and Environment', 'tasks': ['Regression'], 'characteristics': ['Multivariate'], 'num_instances': 517, 'num_features': 12, 'feature_types': ['Real'], 'demographics': [], 'target_col': ['area'], 'index_col': None, 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 2007, 'last_updated': 'Thu Jan 11 2024', 'dataset_doi': '10.24432/C5D88D', 'creators': ['Paulo Cortez', 'Anbal Morais'], 'intro_paper': {'title': 'A data mining approach to predict forest fires using meteorological data', 'authors': 'P. Cortez, AnÃ­bal de Jesus Raimundo Morais', 'published_in': 'New Trends in Artificial Intelligence, Proceedings of the 13th EPIA 2007 - Portuguese Conference on Artificial Intelligence', 'year': 2007, 'url': 'https://www.semanticscholar.org/paper/A-data-mining-approach-to-predict-forest-fires-data-Cortez-Morais/0f529dc2b2b2bad22394454d4cba79e2c319f0b0', 'doi': None}, 'additional_info': {'summary': \"In [Cortez and Morais, 2007], the output 'area' was first transformed with a ln(x+1) function.\\r\\n   Then, several Data Mining methods were applied. After fitting the models, the outputs were\\r\\n   post-processed with the inverse of the ln(x+1) transform. Four different input setups were\\r\\n   used. The experiments were conducted using a 10-fold (cross-validation) x 30 runs. Two\\r\\n   regression metrics were measured: MAD and RMSE. A Gaussian support vector machine (SVM) fed\\r\\n   with only 4 direct weather conditions (temp, RH, wind and rain) obtained the best MAD value:\\r\\n   12.71 +- 0.01 (mean and confidence interval within 95% using a t-student distribution). The\\r\\n   best RMSE was attained by the naive mean predictor. An analysis to the regression error curve\\r\\n   (REC) shows that the SVM model predicts more examples within a lower admitted error. In effect,\\r\\n   the SVM model predicts better small fires, which are the majority. \", 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': \"For more information, read [Cortez and Morais, 2007].\\r\\n   1. X - x-axis spatial coordinate within the Montesinho park map: 1 to 9\\r\\n   2. Y - y-axis spatial coordinate within the Montesinho park map: 2 to 9\\r\\n   3. month - month of the year: 'jan' to 'dec' \\r\\n   4. day - day of the week: 'mon' to 'sun'\\r\\n   5. FFMC - FFMC index from the FWI system: 18.7 to 96.20\\r\\n   6. DMC - DMC index from the FWI system: 1.1 to 291.3 \\r\\n   7. DC - DC index from the FWI system: 7.9 to 860.6 \\r\\n   8. ISI - ISI index from the FWI system: 0.0 to 56.10\\r\\n   9. temp - temperature in Celsius degrees: 2.2 to 33.30\\r\\n   10. RH - relative humidity in %: 15.0 to 100\\r\\n   11. wind - wind speed in km/h: 0.40 to 9.40 \\r\\n   12. rain - outside rain in mm/m2 : 0.0 to 6.4 \\r\\n   13. area - the burned area of the forest (in ha): 0.00 to 1090.84 \\r\\n   (this output variable is very skewed towards 0.0, thus it may make\\r\\n    sense to model with the logarithm transform).\", 'citation': None}}\n",
      "     name     role         type demographic  \\\n",
      "0       X  Feature      Integer        None   \n",
      "1       Y  Feature      Integer        None   \n",
      "2   month  Feature  Categorical        None   \n",
      "3     day  Feature  Categorical        None   \n",
      "4    FFMC  Feature   Continuous        None   \n",
      "5     DMC  Feature      Integer        None   \n",
      "6      DC  Feature   Continuous        None   \n",
      "7     ISI  Feature   Continuous        None   \n",
      "8    temp  Feature   Continuous        None   \n",
      "9      RH  Feature      Integer        None   \n",
      "10   wind  Feature   Continuous        None   \n",
      "11   rain  Feature      Integer        None   \n",
      "12   area   Target      Integer        None   \n",
      "\n",
      "                                          description            units  \\\n",
      "0   x-axis spatial coordinate within the Montesinh...             None   \n",
      "1   y-axis spatial coordinate within the Montesinh...             None   \n",
      "2                  month of the year: 'jan' to 'dec'              None   \n",
      "3                     day of the week: 'mon' to 'sun'             None   \n",
      "4       FFMC index from the FWI system: 18.7 to 96.20             None   \n",
      "5        DMC index from the FWI system: 1.1 to 291.3              None   \n",
      "6          DC index from the FWI system: 7.9 to 860.6             None   \n",
      "7         ISI index from the FWI system: 0.0 to 56.10             None   \n",
      "8                           temperature: 2.2 to 33.30  Celsius degrees   \n",
      "9                      relative humidity: 15.0 to 100                %   \n",
      "10                           wind speed: 0.40 to 9.40             km/h   \n",
      "11                          outside rain: 0.0 to 6.4             mm/m2   \n",
      "12  the burned area of the forest: 0.00 to 1090.84...               ha   \n",
      "\n",
      "   missing_values  \n",
      "0              no  \n",
      "1              no  \n",
      "2              no  \n",
      "3              no  \n",
      "4              no  \n",
      "5              no  \n",
      "6              no  \n",
      "7              no  \n",
      "8              no  \n",
      "9              no  \n",
      "10             no  \n",
      "11             no  \n",
      "12             no  \n"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "forest_fires = fetch_ucirepo(id=162) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = forest_fires.data.features \n",
    "y = forest_fires.data.targets \n",
    "  \n",
    "# metadata \n",
    "print(forest_fires.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(forest_fires.variables) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "columns = [\n",
    "    'coord_x', 'coord_y', 'month', 'day', 'ffmc', 'dmc', 'dc', 'isi', 'temp', 'rh', 'wind', 'rain', 'area' \n",
    "]\n",
    "fires_df = (pd.read_csv('/Users/kristina/python/scaling_to_production/05_src/data/fires/forestfires.csv', header = None, names = columns))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "fires_df = pd.DataFrame({\n",
    "    'feature1': [1, 2, 3, 4, 5],\n",
    "    'feature2': ['A', 'B','A', 'B', 'A'],\n",
    "    'area': [10, 20, 30, 40, 50]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get X and Y\n",
    "\n",
    "Create the features data frame and target data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get features (X) and target (Y)\n",
    "X = fires_df.drop(columns=['area'])  # Features\n",
    "Y = fires_df['area']  # Target\n",
    "#Split data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test =train_test_split(X,Y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Create two [Column Transformers](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html), called preproc1 and preproc2, with the following guidelines:\n",
    "\n",
    "- Numerical variables\n",
    "\n",
    "    * (Preproc 1 and 2) Scaling: use a scaling method of your choice (Standard, Robust, Min-Max). \n",
    "    * Preproc 2 only: \n",
    "        \n",
    "        + Choose a transformation for any of your input variables (or several of them). Evaluate if this transformation is convenient.\n",
    "        + The choice of scaler is up to you.\n",
    "\n",
    "- Categorical variables: \n",
    "    \n",
    "    * (Preproc 1 and 2) Apply [one-hot encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) where appropriate.\n",
    "\n",
    "\n",
    "+ The only difference between preproc1 and preproc2 is the non-linear transformation of the numerical variables.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preproc 1\n",
    "\n",
    "Create preproc1 below.\n",
    "\n",
    "+ Numeric: scaled variables, no other transforms.\n",
    "+ Categorical: one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preproc 1:\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder \n",
    "# Define preprocessing strategies\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "\n",
    "# Preproc 1: Standard scaling for numeric, one-hot encoding for categorical\n",
    "preproc1 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preproc 2\n",
    "\n",
    "Create preproc1 below.\n",
    "\n",
    "+ Numeric: scaled variables, non-linear transformation to one or more variables.\n",
    "+ Categorical: one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preproc2\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PowerTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "# Preproc 2: Standard scaling + PowerTransformer for numeric, one-hot encoding for categorical\n",
    "preproc2 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features), \n",
    "        ('num', StandardScaler(), numeric_features)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Pipeline\n",
    "\n",
    "\n",
    "Create a [model pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html): \n",
    "\n",
    "+ Add a step labelled `preprocessing` and assign the Column Transformer from the previous section.\n",
    "+ Add a step labelled `regressor` and assign a regression model to it. \n",
    "\n",
    "## Regressor\n",
    "\n",
    "+ Use a regression model to perform a prediction. \n",
    "\n",
    "    - Choose a baseline regressor, tune it (if necessary) using grid search, and evaluate it using cross-validation.\n",
    "    - Choose a more advance regressor, tune it (if necessary) using grid search, and evaluate it using cross-validation.\n",
    "    - Both model choices are up to you, feel free to experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline A = preproc1 + baseline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define model pipelines\n",
    "pipeline_A = Pipeline(steps=[\n",
    "    ('preprocessing', preproc1),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline B = preproc2 + baseline\n",
    "\n",
    "pipeline_B = Pipeline(steps=[\n",
    "    ('preprocessing', preproc2),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "pipeline_C = Pipeline(steps=[\n",
    "    ('preprocessing', preproc1),\n",
    "    ('regressor', RandomForestRegressor())\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline D = preproc2 + advanced model \n",
    "\n",
    "pipeline_D = Pipeline([\n",
    "    ('preprocessing', preproc2),\n",
    "    ('regressor', GradientBoostingRegressor())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune Hyperparams\n",
    "\n",
    "+ Perform GridSearch on each of the four pipelines. \n",
    "+ Tune at least one hyperparameter per pipeline.\n",
    "+ Experiment with at least four value combinations per pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grids for GridSearchCV\n",
    " #Define parameter grids\n",
    "params_lr = {\n",
    "    'model__fit_intercept': [True, False],\n",
    "}\n",
    "\n",
    "params_rf = {\n",
    "    'model__n_estimators': [50, 100],\n",
    "    'model__max_depth': [None, 10, 20],\n",
    "}\n",
    "\n",
    "params_gbr = {\n",
    "    'model__n_estimators': [50, 100],\n",
    "    'model__learning_rate': [0.01, 0.1, 0.2],\n",
    "    'model__max_depth': [3, 5, 7],\n",
    "}\n",
    "\n",
    "# Define pipelines\n",
    "pipeline_A = Pipeline(steps=[('preprocessor', preproc1), ('model', LinearRegression())])\n",
    "pipeline_B = Pipeline(steps=[('preprocessor', preproc2), ('model', LinearRegression())])\n",
    "pipeline_C = Pipeline(steps=[('preprocessor', preproc1), ('model', RandomForestRegressor(random_state=42))])\n",
    "pipeline_D = Pipeline(steps=[('preprocessor', preproc2), ('model', GradientBoostingRegressor(random_state=42))])\n",
    "\n",
    "# Define GridSearchCV objects\n",
    "grid_search_lr_A = GridSearchCV(pipeline_A, params_lr, cv=2)\n",
    "grid_search_lr_B = GridSearchCV(pipeline_B, params_lr, cv=2)\n",
    "grid_search_rf_C = GridSearchCV(pipeline_C, params_rf, cv=2)\n",
    "grid_search_gbr_D = GridSearchCV(pipeline_D, params_gbr, cv=2)\n",
    "\n",
    "# Define pipelines list\n",
    "pipelines = [\n",
    "    (grid_search_lr_A, \"Pipeline A (preproc1 + LinearRegression)\"),\n",
    "    (grid_search_lr_B, \"Pipeline B (preproc2 + LinearRegression)\"),\n",
    "    (grid_search_rf_C, \"Pipeline C (preproc1 + RandomForestRegressor)\"),\n",
    "    (grid_search_gbr_D, \"Pipeline D (preproc2 + GradientBoostingRegressor)\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate\n",
    "\n",
    "+ Which model has the best performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline A (preproc1 + LinearRegression) - RMSE: 7.105427357601002e-15\n",
      "Pipeline B (preproc2 + LinearRegression) - RMSE: 7.105427357601002e-15\n",
      "Pipeline C (preproc1 + RandomForestRegressor) - RMSE: 6.600000000000001\n",
      "Pipeline D (preproc2 + GradientBoostingRegressor) - RMSE: 12.421899851439356\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'best_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Save the best model as a pickle file\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 15\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(\u001b[43mbest_model\u001b[49m, f)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluate all models and get the best one\n",
    "def evaluate_models(pipelines, X_train, X_test, Y_train, Y_test):\n",
    "    best_model = None\n",
    "    best_score = float('inf')\n",
    "    \n",
    "for pipeline, name in pipelines:\n",
    "        pipeline.fit(X_train, Y_train)\n",
    "        Y_pred = pipeline.predict(X_test)\n",
    "        rmse = np.sqrt(mean_squared_error(Y_test, Y_pred))\n",
    "        print(f\"{name} - RMSE: {rmse}\")\n",
    "        \n",
    "\n",
    "# Save the best model as a pickle file\n",
    "with open('best_model.pkl', 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The best-performing model in terms of RMSE is Pipeline C (preproc1 + RandomForestRegressor) with an RMSE of 6.6. A lower RMSE indicates better predictive performance, meaning the modelâ€™s predictions are closer to the actual values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export\n",
    "\n",
    "+ Save the best performing model to a pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pipeline_C \n",
    "# Save the model as a pickle file\n",
    "with open('pipeline_c_model.pkl', 'wb') as f:\n",
    "    pickle.dump(pipeline_C, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explain\n",
    "\n",
    "+ Use SHAP values to explain the following only for the best-performing model:\n",
    "\n",
    "    - Select an observation in your test set and explain which are the most important features that explain that observation's specific prediction.\n",
    "\n",
    "    - In general, across the complete training set, which features are the most and least important.\n",
    "\n",
    "+ If you were to remove features from the model, which ones would you remove? Why? How would you test that these features are actually enhancing model performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SHAP values show which features consistently have the biggest impact across the training data.\n",
    "#It provide insights into feature importance for individual predictions. For the best-performing model (Pipeline C with RandomForestRegressor).\n",
    "#To determine overall feature importance we should calculate  absolute SHAP values across the training set, sort features by mean absolute SHAP value and print feature importance ranking\n",
    "#By leveraging SHAP values, we can gain insights into feature importance at both individual prediction and global model levels, enabling informed decisions on feature selection and model refinement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Answer here.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Information\n",
    "\n",
    "ðŸš¨ **Please review our [Assignment Submission Guide](https://github.com/UofT-DSI/onboarding/blob/main/onboarding_documents/submissions.md)** ðŸš¨ for detailed instructions on how to format, branch, and submit your work. Following these guidelines is crucial for your submissions to be evaluated correctly.\n",
    "\n",
    "### Submission Parameters:\n",
    "* Submission Due Date: `HH:MM AM/PM - DD/MM/YYYY`\n",
    "* The branch name for your repo should be: `assignment-3`\n",
    "* What to submit for this assignment:\n",
    "    * This Jupyter Notebook (assignment_3.ipynb) should be populated and should be the only change in your pull request.\n",
    "* What the pull request link should look like for this assignment: `https://github.com/<your_github_username>/production/pull/<pr_id>`\n",
    "    * Open a private window in your browser. Copy and paste the link to your pull request into the address bar. Make sure you can see your pull request properly. This helps the technical facilitator and learning support staff review your submission easily.\n",
    "\n",
    "Checklist:\n",
    "- [ ] Created a branch with the correct naming convention.\n",
    "- [ ] Ensured that the repository is public.\n",
    "- [ ] Reviewed the PR description guidelines and adhered to them.\n",
    "- [ ] Verify that the link is accessible in a private browser window.\n",
    "\n",
    "If you encounter any difficulties or have questions, please don't hesitate to reach out to our team via our Slack at `#cohort-3-help`. Our Technical Facilitators and Learning Support staff are here to help you navigate any challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "Cortez,Paulo and Morais,Anbal. (2008). Forest Fires. UCI Machine Learning Repository. https://doi.org/10.24432/C5D88D."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
